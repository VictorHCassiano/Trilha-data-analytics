# Trilha D&A - AWS [Compass.uol](https://compass.uol/pt/home/)
## Apresentação

- **Nome**: Victor Hugo Cassiano da Silva
- **Cidade de Residência**: Olinda/PE
- **Instituição de Ensino**: UNINASSAU
- **Curso**: Sistemas de Informação
- **Semestre Atual**: 6º Semestre
- **Hobbies**: Jogos, Animes, Mangás e Programação

## Um pouco mais sobre mim
Além dos estudos e da tecnologia, outra grande paixão na minha vida são os jogos, animes e mangás. Desde a infância, essas formas de entretenimento têm sido uma grande  inspiração para mim.

Os jogos oferecem não apenas entretenimento, mas também desafios intelectuais e oportunidades para desenvolver habilidades estratégicas. Desde RPGs imersivos até jogos de estratégia, eu amo a diversidade de experiências que os jogos proporcionam. Além disso, a comunidade de jogadores é uma fonte de conexões e amizades valiosas, e participar de eventos e torneios locais é uma experiência que sempre aguardo com entusiasmo, trazendo uma importante experiencia de trabalho em equipe, além de momentos históricos com seus amigos.

Essas paixões não são apenas fontes de entretenimento, mas também influenciam minha abordagem à tecnologia e ao design. Ao observar os mundos e mecânicas dos jogos, encontro inspiração para criar soluções inovadoras e envolventes em meus próprios projetos.

## Sprint-1: Git/Github/Linux
Git/Github: Comandos basicos, git add, init, commit, pull,push desfazer commits, como fazer um repositorio, funcionalidades do github etc.
Linux:

- [Exercício 1](https://github.com/VictorHCassiano/Trilha-data-analytics/blob/main/Sprint-1/exercicios/ex1.md)
- [Exercício 2](https://github.com/VictorHCassiano/Trilha-data-analytics/blob/main/Sprint-1/exercicios/ex2.md)
- [Exercício 3](https://github.com/VictorHCassiano/Trilha-data-analytics/blob/main/Sprint-1/exercicios/ex3.md)
- [Exercício 4](https://github.com/VictorHCassiano/Trilha-data-analytics/blob/main/Sprint-1/exercicios/ex4.md)

### Certificados

[Link para o Certificado Git e GitHub](link_do_certificado)

---


## Sprint 2: Big Data e SQL para Análise de Dados

### Big Data:

- **Conceitos Fundamentais:**
  - Exploramos os princípios fundamentais do Big Data, compreendendo os "Vs" essenciais: Volume, Velocidade, Variedade, Veracidade e Valor.
  
- **Tecnologias de Armazenamento:**
  - Adquirimos conhecimento sobre as diferentes tecnologias de armazenamento utilizadas em ambientes de Big Data, como Hadoop Distributed File System (HDFS) e Apache HBase.
  
- **Processamento Distribuído:**
  - Entendemos os conceitos de processamento distribuído para lidar eficientemente com grandes conjuntos de dados, explorando ferramentas como Apache Spark.

### SQL para Análise de Dados:

- **Consulta Básica:**
  - Aprofundamos nossas habilidades em consultas SQL básicas, abrangendo seleção, projeção, junção e ordenação de dados.

- **Funções Analíticas:**
  - Exploramos funções analíticas avançadas no SQL para realizar análises mais complexas, como agregações, particionamento e classificação de dados.

- **Otimização de Consultas:**
  - Aprendemos estratégias para otimizar consultas SQL, visando melhor desempenho na recuperação e manipulação de grandes conjuntos de dados.

### Sprint 3: Python - Estruturas Avançadas e Desenvolvimento de Aplicações

Durante o terceiro sprint, exploramos aprofundadamente conceitos avançados em Python, abrangendo uma variedade de tópicos essenciais para o desenvolvimento de aplicações robustas. Desde estruturas de controle até o isolamento eficiente de ambientes, cada área foi minuciosamente abordada para ampliar nossa proficiência na linguagem.

- **Estruturas de Controle**:

    - Aprimoramos nossa compreensão das estruturas de controle em Python, explorando técnicas avançadas para manipulação eficiente do fluxo do programa. Isso incluiu o aprofundamento em estruturas como `if-else`, `for` e `while`, permitindo uma abordagem mais sofisticada na lógica de programação.

-  **Manipulação de Arquivos**:

    - Aprofundamos nossas habilidades na manipulação eficaz de arquivos em Python, abordando não apenas a leitura e gravação de dados, mas também estratégias avançadas para lidar com diferentes tipos e formatos de arquivos.

- **Comprehension e Funções**:

    - Exploramos as capacidades avançadas do Python usando compreensões, tanto de listas quanto de dicionários, para escrever código mais conciso e expressivo. Além disso, aprimoramos nossas habilidades na criação e uso de funções, tornando nosso código modular e fácil de manter.

- **Programação Orientada a Objetos Avançada**:

    - Expandimos nosso conhecimento em programação orientada a objetos, explorando conceitos avançados como herança, polimorfismo e encapsulamento. Isso fortaleceu nossa capacidade de projetar e implementar sistemas mais complexos.

 - **Gerenciamento de Pacotes e Isolamento de Ambientes**:

    - Aprendemos a importância do gerenciamento de pacotes e exploramos técnicas avançadas para garantir o isolamento eficiente de ambientes usando ferramentas como virtualenv. Isso é crucial para o desenvolvimento de aplicações escaláveis e de fácil manutenção.

### Sprint 4: Programação Funcional em Python, Docker Avançado com Docker Swarm e Kubernetes, e Estatística Descritiva em Python

Durante o quarto sprint, mergulhamos em tópicos avançados que ampliaram significativamente nossa base de conhecimento em Python, com ênfase na programação funcional, na orquestração de contêineres com Docker Swarm e Kubernetes, e na aplicação de estatística descritiva em nossos projetos.

- **Programação Funcional em Python**:

    - Exploramos os princípios da programação funcional em Python, destacando conceitos como funções de ordem superior, expressões lambda, e map/reduce/filter. Essa abordagem nos permitiu escrever código mais conciso, modular e aproveitar os benefícios da imutabilidade.

- **Docker Avançado com Docker Swarm e Kubernetes**:

    - Aprofundamos nosso entendimento sobre contêineres, estendendo-o para abranger ambientes mais complexos. Investigamos as capacidades do Docker Swarm para orquestrar múltiplos contêineres, proporcionando escalabilidade e tolerância a falhas. Além disso, adentramos o universo do Kubernetes, explorando sua arquitetura, recursos avançados e a orquestração eficaz de aplicações em larga escala.

- **Estatística Descritiva em Python**:

    - Aplicamos conceitos estatísticos descritivos utilizando Python, aprimorando nossa capacidade de analisar e interpretar dados. Exploramos medidas de tendência central, dispersão e visualização de dados, proporcionando uma base sólida para a tomada de decisões informadas em nossos projetos.



### Sprint 5: AWS - Certificações e Treinamentos

Durante a quinta sprint, concentramos nossos esforços em aprimorar nossos conhecimentos na plataforma AWS (Amazon Web Services), obtendo certificações e participando de treinamentos especializados.

- **AWS Partner Accreditation Technical Portuguese.pdf**:
    - Certificação técnica de parceiro AWS em português, aprimorando nosso entendimento das soluções técnicas oferecidas pela AWS.

- **AWS Partner Acreditação de economias na nuvem AWS – treinamento de vendas.pdf**:
    - Treinamento de vendas com foco nas economias proporcionadas pelas soluções em nuvem da AWS, fortalecendo nossas habilidades na apresentação e venda de serviços AWS.

- **AWS Partner Sales Accreditation (Business).pdf**:
    - Certificação de vendas para parceiros AWS, aprofundando nosso conhecimento nas estratégias de vendas relacionadas aos serviços de negócios da AWS.

- **Cloud quest certificado.png**:
    - Certificado Cloud Quest, evidenciando nossa participação e sucesso em desafios relacionados à nuvem.

- **Exam Prep AWS Certified Cloud Practitioner (Portuguese).pdf**:
    - Preparação para o exame AWS Certified Cloud Practitioner em português, consolidando nosso entendimento dos conceitos fundamentais da AWS.


### Sprint 6: AWS - Data Analytics e Best Practices

Durante a sexta sprint, aprofundamos nossos conhecimentos na área de Data Analytics na AWS (Amazon Web Services), explorando cursos especializados e adotando as melhores práticas para otimizar nossas estratégias de análise de dados.

- **AWS Partner Data Analytics on AWS (Business).pdf**:
  - Curso de Análise de Dados na AWS para Parceiros, focado em aspectos de negócios, fortalecendo nossa capacidade de oferecer soluções analíticas para clientes.

- **Best Practices for Data Warehousing with Amazon Redshift (Portuguese).pdf**:
  - Curso em português sobre as melhores práticas para Armazenamento de Dados com o Amazon Redshift, aprimorando nossa habilidade na construção e otimização de data warehouses.

- **Data Analytics Fundamentals (portuguese).pdf**:
  - Fundamentos da Análise de Dados em português, proporcionando uma base sólida para aplicação prática em projetos de análise na AWS.

- **Deep Dive into Concepts and Tools for Analyzing Streaming Data (Portuguese).pdf**:
  - Curso aprofundado em português sobre conceitos e ferramentas para análise de dados em tempo real, preparando-nos para lidar eficientemente com fluxos contínuos de dados.

- **Getting Started with Amazon Redshift.pdf**:
  - Guia de iniciação ao Amazon Redshift, oferecendo uma visão prática e hands-on para começar a trabalhar com este serviço de armazenamento de dados.

- **Introduction to AWS IoT Analytics.pdf**:
  - Introdução à Análise de Dados na Internet das Coisas (IoT) na AWS, ampliando nosso conhecimento sobre análise de dados em ambientes IoT.

- **Introduction to Amazon Athena (Portuguese).pdf**:
  - Curso em português apresentando o Amazon Athena, aprimorando nossa habilidade na execução de consultas SQL em dados armazenados no Amazon S3.

- **Introduction to Amazon Elastic MapReduce (EMR) (Portuguese).pdf**:
  - Introdução ao Amazon Elastic MapReduce (EMR) em português, abordando conceitos e práticas para processamento distribuído de grandes conjuntos de dados.

- **Introduction to Amazon Kinesis Analytics.pdf**:
  - Curso introdutório sobre o Amazon Kinesis Analytics, fornecendo uma compreensão detalhada das capacidades analíticas deste serviço de streaming.

- **Introduction to Amazon Kinesis Streams.pdf**:
  - Introdução ao Amazon Kinesis Streams, explorando os conceitos por trás do processamento de streaming e suas aplicações práticas.

- **Introduction to Amazon Quicksight (Portuguese).pdf**:
  - Curso em português apresentando o Amazon Quicksight, aprimorando nossa capacidade de criar visualizações de dados interativas e informativas.

- **Serverless Analytics (Portuguese).pdf**:
  - Curso em português sobre análise de dados sem servidor, explorando abordagens e práticas inovadoras na execução de análises na nuvem.

- **Why Analytics for Games (Portuguese).pdf**:
  - Curso em português abordando a importância e as estratégias de análise de dados em jogos, alinhando-se aos nossos interesses pessoais e profissionais.

### Sprint 7: Hadoop, PySpark e Desafio - Parte 1

Durante a sétima sprint, focamos em consolidar nossos conhecimentos em Big Data, com ênfase no Hadoop e PySpark. Iniciamos a primeira parte do desafio, que consiste em transferir os arquivos "movies" e "series" para a nuvem, utilizando o serviço S3 da AWS.

- **Curso Básico de Hadoop**:
  - Exploração dos fundamentos do Hadoop, incluindo conceitos-chave como HDFS (Hadoop Distributed File System) e processamento distribuído. Este curso proporcionou uma compreensão sólida dos princípios essenciais do ecossistema Hadoop.

- **Curso Completo para PySpark**:
  - Imersão profunda no PySpark, abrangendo desde os conceitos básicos até técnicas avançadas. Este curso fortaleceu nossa capacidade de utilizar o PySpark para processamento de dados em ambientes distribuídos.

- **Desafio - Parte 1**:

    - **Objetivo**: Transferir os arquivos "movies" e "series" para o serviço S3 da AWS.

    - **Descrição do Desafio**:
        - Utilizar o a biblioteca boto3 para enviar os arquivos para a nuvem onde deveria ser armazenado em um S3 , organizando os dados da seguinte forma, /Raw/Local/CSV/ano/mes/dia/

### Sprint 8: Desafio - Parte 2
- **Desafio - Parte 2**:

    - **Objetivo**: Ingestão de dados do TMBD 

    - **Descrição do Desafio**:
        - Compreende a captura de dados existentes na API TMBD. Os dados coletados devem ser persistidos em Amazon S3 RAW Zone, mantendo o formato da origem (JSON), agrupando-os em arquivos 


### Sprint - 9 Desafio - Parte 3
- **Desafio - Parte 3**:

    - **Descrição do Desafio**:
        - Criacao da camada trusted e refined para o consumo de dados na dashboard


### Sprint - 10 Desafio - Final
- **Desafio - Final**:

    - **Descrição do Desafio**:
        - Criacao da dashboard